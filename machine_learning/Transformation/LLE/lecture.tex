%Fiquemos com Deus e Nossa Senhora!
%Sao Jose de Cupertino rogai por nos que recorremos a vos!!!
%Honra teu Pai e tua Mãe!
% ### Uses XeLaTeX ### %
% ### Needs beamer-master ### %
\documentclass[aspectratio=169]{beamer} %. Aspect Ratio 16:9

\usetheme{AI2} % beamerthemeSprace.sty
\usepackage{algpseudocodex}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ragged2e,gensymb,bm,amsmath,amssymb}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sign}{sgn}

% DATA FOR FOOTER
\date{2021}
\title{- LLE}
\institute{Advanced Institute for Artificial Intelligence (AI2)}

\begin{document}
% ####################################
% FIRST SLIDE 						:: \SliTit{This is the Title of the Talk}{A. B. Name}{Sprace}
% SUB-TITLE SLIDE 					:: \SliSubTit{<title>}{<explanation}
% SUB-SUB-TITLE SLIDE				:: \SliSubSubTit{<title>}{<explanation}
% SLIDE WITH TITLE 					:: \SliT{Title}{Content}
% SLIDE NO TITLE 						:: \Sli{Content} 
% SLIDE DOUBLE COLUMN WITH TITLE 	:: \SliDT{Title}{First Column}{Second Column}
% SLIDE DOUBLE COLUMN NO TITLE 		:: \SliD{First Column}{Second Column}
% SLIDE ADVANCED WITH TITLE 			:: \SliAdvT{Title}{Content}
% SLIDE ADVANCED NO TITLE 			:: \SliAdv{Content}
% SLIDE ADVANCED DOUBLE WITH TITLE 	:: \SliAdvDT{Title}{First Column}{Second Column}
% SLIDE ADVANCED DOUBLE NO TITLE 	:: \SliAdvD{First Column}{Second Column}
% SLIDE BLACK						:: \Black{ <Content> }
% SLIDE WHITE						:: \White{ <Content> }
% ITEMIZATION 						:: \begin{itemize}  \iOn{First} \iTw {Second} \iTh{Third} \end{itemize}
% COMMENT TEXT				 		:: \note{<comment>}
% SECTION 							:: \secx{Section} | \secxx{Sub-Section}
% BOLD SPRACE COLOR				:: \bfs{<text>}
% TABLE OF CONTENT					:: \tocitem{<title>}{<content>}
% LEFT ALIGN EQUATION				:: \begin{flalign*}  & <equation> &   \end{flalign*}
% CENTER ALIGN EQUATION	S			:: \begin{gather*} <equations>  \end{gather*}
% SLASH								:: \slashed{<>}
% BAR								:: \barr{<letter>} instead of \bar{<letter>}
% THEREFORE						:: use \portanto (larger and bold}
% 2 or 3 MATH SYMBOLS				:: \overset{<up>}{<down>} &  \underset{<below>}{\overset{<above>}{<middle>}}  
% INSERT TEXT IN FORMULA			:: \ins{<text>}
% EXERCISE							:: \exe{<exercise #>}{<exercise text>}
% SUGGESTED READING BOX			:: \sug{<references>}
% CITATION							:: \cittex{<citation>}
% CITATION DOUBLE COLUMN 			:: \cittexD{<citation>}
% TEXT POSITION						:: \texpos{<Xcm>}{<Ycm>}{<text>} origin = center of slide : x right | y down
% REFERENCE AT BOTTOM  S/D SLIDE		:: \refbotS{<reference>} \refbotD{<reference>}
% HIDDEN SLIDE						:: \hid
% COLOR BOX 						:: \blu{blue} + \red{rec} + \yel{yellow} + \gre{green} + \bege{beige}
% FRAME 							:: \fra{sprace} \frab{blue} \frar{red} + \fray{yellow} + \frag{green}		
% FIGURE 							:: \img{X}{Y}{<scale>}{Figure.png} 
% FIGURE							:: \includegraphics[scale=<scale>]{Figures/.png}
% FIGURE DOUBLE SLIDE NO TITLE		::  \img{-4}{0.5}{<scale>}{Figure.png} % Image 1st half
%									::  \img{4}{0.5}{<scale>}{Figure.png} % Image 2nd half
% FIGURE DOUBLE SLIDE WITH TITLE		::  \img{-4}{0}{<scale>}{Figure.png} % Image 1st half
%									::  \img{4}{0}{<scale>}{Figure.png} % Image 2nd half
% INCLUDING SWF (Flash)				:: \usepackage{media9} and \includemedia >> USE ACROBAT <<
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
% FIRST SLIDE
\SliTit{{\LARGE LLE - Locallly Linear Embedding}}{Advanced Institute for Artificial Intelligence -- AI2}{https://advancedinstitute.ai}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
% SLIDE SUB-TITLE
%\SliSubTit{Sub-Title}{Description}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
%\SliSubSubTit{Sub-Sub-Title}{Description}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\SliT{Introdução}{

\justifying A técnica \emph{Locally Linear Embedding} (LLE) é uma técnica \textbf{local} para redução não linear de dimensionalidade que baseia-se no arcabouço de \textbf{aprendizado de variedades} (\emph{manifold} learning). A principal diferença desse método para ISOMAP é que este é uma técnica \textbf{global}, pois usávamos a matriz de distâncias entre \textbf{todas as amostras} do conjunto de dados para aprendermos o novo espaço reduzido. No caso do LLE, iremos considerar apenas a vizinhança (\emph{patch}) de um dado ponto para aprendermos a sua nova coordenada no espaço reduzido.\newline

\justifying \underline{Hipótese}: caso tomemos uma amostra e os seus vizinhos mais próximos, teremos um \emph{patch} linear (espaço Euclidiano), ou seja, eu posso aproximar o elemento central (amostra) por meio da \textbf{combinação linear} dos seus vizinhos.
}

\Sli{
\justifying Seja ${\cal X}=\{\bm{x}_1,\bm{x}_2,\ldots,\bm{x}_m\}$ o nosso conjunto de dados original tal que $\bm{x}_i\in\mathbb{R}^n$. Temos, então que a nova amostra projetada $\hat{\bm{x}}_i$ é obtida da seguinte forma:

\begin{equation}
	\hat{\bm{x}}_i \approx \sum_{j}w_{ij}\bm{x}_j,\ \forall \bm{x}_j\in{\cal N}(\bm{x}_i),
\end{equation}
em que ${\cal N}(\bm{x}_i)$ denota a vizinhança (\emph{patch}) da amostra $\bm{x}_i$ e $w_{ij}$ o peso atribuído durante a combinação linear. O algoritmo do LLE requer, basicamente, o conjunto de dados representado por uma matriz $\bm{X}\in\mathbb{R}^{m\times n}$, o número desejado de dimensões $d<m$ e um valor $k>d+1$ para o tamanho da vizinhança. Sem perda de generalidade, vamos assumir que $\bm{X}_i = \bm{x}_i$, ou seja, a $i$-ésima linha da matrix $\bm{X}$ corresponde à $i$-ésima amostra de ${\cal X}$.\newline

\justifying O algoritmo do LLE pode ser dividido em três passos principais:
}

\Sli{
\begin{enumerate}
	\item Para cada $\bm{x}_i\in{\cal X}$ encontrar os seus $k$-vizinhos mais próximos, $i=1,2,\ldots,m$.
	\item Encontrar a matrix $\bm{W}$ que minimiza o erro de reconstrução para cada amostra $\bm{x}_i\in{\cal X}$:
		\begin{equation}
		E(\bm{W},\bm{X}) = \sum_{i=1}^m\norm{\bm{x}_i-\sum_jw_{ij}\bm{x}_j}^2,\ \forall \bm{x}_j\in{\cal N}(\bm{x}_i),
		\end{equation}
		em que $w_{ij} = 0$ a menos que $\bm{x}_j\in{\cal N}(\bm{x}_i)$. Note que $\sum_jw_{ij}=1$.
	\item Encontre o conjunto de coordenadas $\hat{\bm{X}}$ que minimiza o erro de reconstrução utilizando os pesos ótimos encontrados no passo anterior:
		\begin{equation}
		\Phi(\bm{W},\hat{\bm{X}}) = \sum_{i=1}^m\norm{\hat{\bm{x}}_i-\sum_jw_{ij}\hat{\bm{x}}_j}^2,\ \forall \hat{\bm{x}}_j\in{\cal N}(\hat{\bm{x}}_i),
		\end{equation}
\end{enumerate}
tal que $\hat{\bm{x}}\in\mathbb{R}^d$ e $\hat{\bm{X}}\hat{\bm{X}}^T=\bm{I}$. Sem perda de generalidade, vamos assumir que $\hat{\bm{X}}_i = \hat{\bm{x}}_i$, ou seja, a $i$-ésima linha da matrix $\hat{\bm{X}}$ corresponde à $i$-ésima amostra de $\hat{{\cal X}}$.
}

\Sli{
\justifying O primeiro passo corresponde à encontrar os $k$-vizinhos mais próximos de $\bm{x}_i$, $\forall \bm{x}_i\in{\cal X}$. Em geral, valores pequenos de $k$ tornam o grafo desconexo, enquanto que um $k>n$ exige uma \textbf{regularização} do problema.\newline

\justifying Agora, vamos para o passo 2, ou seja, estimação dos pesos $\bm{W}$ utilizando os mínimos quadrados. Sem perda de generalidade, podemos expressar o erro de reconstrução local considerando apenas o ponto $\bm{x}_i\in{\cal X}$ como segue:

\begin{align}\nonumber
	E(\bm{w},\bm{x}_i) &= \norm{\bm{x}_i-\sum_jw_j\bm{x}_j}^2 = \norm{\cancelto{1\text{ (restrição)}}{\sum_jw_j}\bm{x}_i-\sum_jw_j\bm{x}_j}^2 = \norm{\sum_jw_j(\bm{x}_i-\bm{x}_j)}^2\\ 
	&= \sum_j\sum_kw_jw_k(\bm{x}_i-\bm{x}_j)^T(\bm{x}_i-\bm{x}_k).
\end{align}
}

\Sli{
\justifying Vamos, agora, definir a matriz $\bm{C}\in\mathbb{R}^{n\times n}$:

\begin{equation}
	C_{jk} = (\bm{x}_i-\bm{x}_j)^T(\bm{x}_i-\bm{x}_k).
\end{equation}
Desta forma, podemos reescrever a expressão para o erro local de reconstrução (Equação 4) como segue:

\begin{equation}
	E(\bm{w},\bm{x}_i) = \sum_j\sum_kw_jC_{jk}w_k = \bm{w}^T\bm{C}\bm{w}.
\end{equation}
Lembrando que o intuito principal da restrição $\sum_jw_j=1$ é para adicionar propriedades de \textbf{invariância à translação}, isto é, adicionando uma constante $\bm{c}\in\mathbb{R}^n$ ao vetor $\bm{x}_i$ e aos seus vizinhos, o erro de reconstrução não muda.
}

\Sli{
\justifying Sejam, então, $\tilde{\bm{x}_i} = \bm{x}_i+\bm{c}$ e $\tilde{\bm{x}_j} = \bm{x}_j+\bm{c}$ as versões deslocadas das amostras $\bm{x}_i$ e $\bm{x}_j$, respectivamente. O novo erro de reconstrução local é dado por:

\begin{align}\nonumber
	\tilde{E}(\bm{w},\bm{x}_i) &= \norm{\tilde{\bm{x}_i}-\sum_jw_j\tilde{\bm{x}_j}}^2 = \norm{\bm{x}_i+\bm{c}-\sum_jw_j(\bm{x}_j+\bm{c})}^2\\
	&= \norm{\bm{x}_i+\bm{c}-\sum_jw_j\bm{x}_j-\sum_jw_j\bm{c}}^2 = \norm{\bm{x}_i+\bm{c}-\sum_jw_j\bm{x}_j-\bm{c}\cancelto{1}{\sum_jw_j}}^2\\
	&= \norm{\bm{x}_i+\bm{c}-\sum_jw_j\bm{x}_j-\bm{c}}^2 = \norm{\bm{x}_i-\sum_jw_j\bm{x}_j}^2 = E(\bm{w},\bm{x}_i)\nonumber.
\end{align}
Assim, a translação não afeta o erro quando temos a restrição $\sum_jw_j=1$.
}

\Sli{
\justifying Note que a Equação 6 vale para apenas uma variável $\bm{x}_i\in{\cal X}$. Precisamos, então, generalizá-la para todo o conjunto de dados, pois assim teremos $m$ problemas de otimização com restrição de igualdade, ou seja:

\begin{eqnarray}
	\bm{w}_i^\ast = \argmin_{\bm{w}_i}\bm{w}_i^T\bm{C}_i\bm{w}_i,\\
	\text{s.a. }\bm{1}^Tw_i = 1,\ i=1,2,\ldots,m.\nonumber
\end{eqnarray}
\justifying Como temos um problema de otimização com restrições, precisamos fazer uso da técnica dos multiplicadores de Lagrange. Desta forma, a Equação 8 pode ser reformulada da seguinte maneira:

\begin{equation}
	L(\bm{w}_i,\lambda) = \bm{w}_i^T\bm{C}_i\bm{w}_i-\lambda\left(\bm{1}^T\bm{w}_i-1\right).
\end{equation}
}

\Sli{
\justifying Para resolvermos o problema de otimização, basta calcular a derivada da Equação 9 em relação à $\bm{w}_i$ e igualar à $0$, ou seja:

\begin{equation}
	\frac{\partial L(\bm{w}_i,\lambda)}{\partial\bm{w}_i} = 2\bm{C}_i\bm{w}_i-\lambda\bm{1}^T = 0.
\end{equation}
Da equação acima, temos que:

\begin{equation}
	\bm{C}_i\bm{w}_i = \frac{\lambda}{2}\bm{1}.
\end{equation}
Caso a matriz $\bm{C}_i$ seja admita inversa, temos uma equação fechada para nosso problema de otimização:

\begin{equation}
	\bm{w}_i = \frac{\lambda}{2}\bm{C}^{-1}_i\bm{1}.
\end{equation}
}

\Sli{
\justifying Usualmente, descartamos o termo $\frac{\lambda}{2}$ da Equação 12, pois é uma constante (não depende de $i$). Assim sendo, podemos reescrever a Equação 12 da seguinte forma:

\begin{equation}
	\bm{C}_i\bm{w}_i = \bm{1},
\end{equation}
que é a formulação de um sistema linear. Desta forma, basta resolvermos este sistema utilizando alguma técnica qualquer e, após encontrar $\bm{w}_i$, realizamos a sua normalização, ou seja:

\begin{equation}
	w_{ij} = \frac{w_{ij}}{\sum_jw_{ij}}.
\end{equation}
Desta forma, garantimos que a restrição $\sum_j\bm{w}_i=1$ será satisfeita.
}

\Sli{
\justifying Feito isso, resolvemos o subproblema 1 para o \textbf{caso padrão}, ou seja, quando $k<n$. Quando temos a situação oposta, ou seja, $k>n$, precisamos \textbf{regularizar} o nosso problema, pois o mesmo fica \textbf{mal condicionado}, ou seja, existem mais variáveis desconhecidas ($k$) do que o número de equações para resolvê-las ($n$).\newline

\justifying Uma técnica bastante comum é a regularização de Tikonov, ou seja, ao invés de minimizar diretamente a equação abaixo:

\begin{equation}
	E(\bm{w}) = \norm{\bm{x}_i-\sum_jw_j\bm{x}_j}^2,
\end{equation}
adicionamos um termo de regularização:

\begin{equation}
	E(\bm{w}) = \norm{\bm{x}_i-\sum_jw_j\bm{x}_j}^2+\alpha\sum_{j}w_j^2.
\end{equation}
}

\Sli{
\justifying Na equação anterior, temos que $\alpha$ controla o grau de regularização. A ideia é encontrar uma relação custo-benefício para o valor de $\alpha$, ou seja, quando $\alpha\rightarrow0$, temos que o problema torna-se o de mínimos quadrados. Quando $\alpha\rightarrow\infty$, o termo de regularização "pesa"\ bastante no processo de otimização, ou seja, queremos minimizar a norma Euclidiana do vetor $\bm{w}$ de tal forma a anular o termo de regularização. Tipicamente, utilizamos valores de $\alpha>0$, porém pequenos.\newline

\justifying Neste caso, o problema de otimização passa a ser o seguinte:

\begin{eqnarray}
	\bm{w}_i^\ast = \argmin_{\bm{w}_i}\bm{w}_i^T\bm{C}_i\bm{w}_i+\alpha\bm{w}_i^T\bm{w}_i,\\
	\text{s.a. }\bm{1}^Tw_i = 1,\ i=1,2,\ldots,m.\nonumber
\end{eqnarray}
}

\Sli{
\justifying A função Lagrangiana referente à Equação 17 é dada por:

\begin{equation}
	L(\bm{w}_i,\lambda) = \bm{w}_i^T\bm{C}_i\bm{w}_i+\alpha\bm{w}_i^T\bm{w}_i-\lambda\left(\bm{1}^T\bm{w}_i-1\right).
\end{equation}
Tomando a sua derivada com relação à $\bm{w}_i$ e igualando à $0$, temos que:

\begin{equation}
	\frac{\partial L(\bm{w}_i,\lambda)}{\partial\bm{w}_i} = 2\bm{C}_i\bm{w}_i+2\alpha\bm{w}_i=\lambda\bm{1}^T = 0.
\end{equation}
Podemos reescrever a equação acima da seguinte forma:

\begin{equation}
	(\bm{C}_i+\alpha\bm{I})\bm{w}_i = \frac{\lambda}{2}\bm{1}.
\end{equation}
}

\Sli{
\justifying Agora temos uma equação fechada para encontrar $\bm{w}_i$, ou seja: 

\begin{equation}
	\bm{w}_i = \frac{\lambda}{2}(\bm{C}_i+\alpha\bm{I})^{-1}\bm{I}.
\end{equation}
Podemos descartar, novamente, o termo $\frac{\lambda}{2}$, resultando no seguinte sistema linear:

\begin{equation}
	(\bm{C}_i+\alpha\bm{I})\bm{w}_i = \bm{1}.
\end{equation}
A diferença para a abordagem sem regularização é que estamos adicionando uma leve perturbação na diagonal principal da matriz $\bm{C}_i$ por conta do termo $\alpha\bm{I}$.
}

\Sli{
\justifying Agora estamos no \textbf{subproblema 2}, ou seja, dado que temos os pesos de reconstrução, queremos saber quem são as coordenadas no espaço de menor dimensão que minimizam o erro quadrático (Equação 3). Este segundo problema de otimização possui duas restrições:

\begin{enumerate}
	\item A média dos dados no espaço transformado $\hat{\bm{X}}$ é zero, ou seja:
	\begin{equation}
		\hat{\bm{\mu}} = \frac{1}{m}\sum_{i=1}^m\hat{\bm{x}}_i = 0.
	\end{equation}
	\item A matriz de covariância dos dados transformados é a matriz de identidade, ou seja, não existe correlação entre os componentes de $\hat{\bm{x}}\in\mathbb{R}^d$.
\end{enumerate}
}

\Sli{
\justifying Ao contrário do passo anterior, aqui não conseguimos separar o problema em $m$ subproblemas independentes. Da Equação 3, temos que:

\begin{align}\nonumber
		\Phi(\hat{\bm{X}}) &= \sum_{i=1}^m\norm{\hat{\bm{x}}_i-\sum_jw_{ij}\hat{\bm{x}}_j}^2\\
		&= \sum_{i=1}^m\left[\left(\hat{\bm{x}}_i-\sum_jw_{ij}\hat{\bm{x}}_j\right)^T\left(\hat{\bm{x}}_i-\sum_jw_{ij}\hat{\bm{x}}_j\right)\right].
\end{align}
}

\Sli{
\justifying Aplicando a distributiva na Equação 24, temos que:

\begin{equation}
\begin{split}
	\Phi(\hat{\bm{X}}) &= \sum_{i=1}^m\left[\hat{\bm{x}}_i^T\hat{\bm{x}}_i-\hat{\bm{x}}_i^T\sum_jw_{ij}\hat{\bm{x}}_j-\left(\sum_jw_{ij}\hat{\bm{x}}_j\right)^T\hat{\bm{x}}_i+\right.\\
		&+ \left.\left(\sum_jw_{ij}\hat{\bm{x}}_j\right)^T\left(\sum_jw_{ij}\hat{\bm{x}}_j\right)\right].
\end{split}
\end{equation}
}

\Sli{
\justifying Expandindo o somatório, temos que:

\begin{equation}
\begin{split}
	\Phi(\hat{\bm{X}}) &= \sum_{i=1}^m\hat{\bm{x}}_i^T\hat{\bm{x}}_i-\sum_{i=1}^m\sum_jw_{ij}\hat{\bm{x}}_i^T\hat{\bm{x}}_j-\sum_{i=1}^m\sum_jw_{ji}\hat{\bm{x}}_j^T\hat{\bm{x}}_i+\\
		&+ \sum_{i=1}^m\sum_j\sum_kw_{ji}\hat{\bm{x}}_j^Tw_{ik}\hat{\bm{x}}_k.
\end{split}
\end{equation}
}

\Sli{
Podemos reescrever a equação acima utilizando traços (soma dos elementos das diagonais) de matrizes, ou seja:

\begin{align}\nonumber
	\Phi(\hat{\bm{X}}) &= \Tr(\hat{\bm{X}}^T\hat{\bm{X}})-\Tr(\hat{\bm{X}}^T\bm{W}\hat{\bm{X}})-\Tr(\hat{\bm{X}}^T\bm{W}^T\hat{\bm{X}})+\Tr(\hat{\bm{X}}^T\bm{W}^T\bm{W}\hat{\bm{X}})\\
	&= \Tr(\hat{\bm{X}}^T\hat{\bm{X}})-\Tr(\hat{\bm{X}}^T(\bm{W}\hat{\bm{X}}))-\Tr((\bm{W}\hat{\bm{X}})^T\hat{\bm{X}})+\Tr((\bm{W}\hat{\bm{X}})^T(\bm{W}\hat{\bm{X}}))\\\nonumber
	&= \Tr(\hat{\bm{X}}^T\hat{\bm{X}}-\hat{\bm{X}}^T(\bm{W}\hat{\bm{X}})-(\bm{W}\hat{\bm{X}})^T\hat{\bm{X}}+(\bm{W}\hat{\bm{X}})^T(\bm{W}\hat{\bm{X}}))\\\nonumber
	&= \Tr(\hat{\bm{X}}^T(\hat{\bm{X}}-\bm{W}\hat{\bm{X}})-(\bm{W}\hat{\bm{X}})^T(\hat{\bm{X}}-\bm{W}\hat{\bm{X}}))\\\nonumber
	&= \Tr((\hat{\bm{X}}-\bm{W}\hat{\bm{X}})^T(\hat{\bm{X}}-\bm{W}\hat{\bm{X}}))\\\nonumber
	&= \Tr(((\bm{I}-\bm{W})\hat{\bm{X}})^T((\bm{I}-\bm{W})\hat{\bm{X}}))\\\nonumber
	&= \Tr(\hat{\bm{X}}^T(\bm{I}-\bm{W}^T)(\bm{I}-\bm{W})\hat{\bm{X}})
\end{align}
}

\Sli{
\justifying Um agradecimento especial ao \textbf{Prof. Alexandre Levada} do Centro de Ciências Exatas e de Tecnologia, Departamento de Computação, Universidade Federal de São Carlos, pelas notas de aula.
}

\end{document}