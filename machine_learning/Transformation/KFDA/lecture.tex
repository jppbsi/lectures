%Fiquemos com Deus e Nossa Senhora!
%Sao Jose de Cupertino rogai por nos!!
%Honra teu Pai e tua Mãe!
% ### Uses XeLaTeX ### %
% ### Needs beamer-master ### %
\documentclass[aspectratio=169]{beamer} %. Aspect Ratio 16:9

\usetheme{AI2} % beamerthemeSprace.sty
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ragged2e,gensymb,bm,amsmath,amssymb}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sign}{sgn}

% DATA FOR FOOTER
\date{2021}
\title{- Kernel Fisher Discriminant Analysis}
\author{João Paulo Papa}
\institute{Advanced Institute for Artificial Intelligence (AI2)}

\begin{document}
% ####################################
% FIRST SLIDE 						:: \SliTit{This is the Title of the Talk}{A. B. Name}{Sprace}
% SUB-TITLE SLIDE 					:: \SliSubTit{<title>}{<explanation}
% SUB-SUB-TITLE SLIDE				:: \SliSubSubTit{<title>}{<explanation}
% SLIDE WITH TITLE 					:: \SliT{Title}{Content}
% SLIDE NO TITLE 						:: \Sli{Content} 
% SLIDE DOUBLE COLUMN WITH TITLE 	:: \SliDT{Title}{First Column}{Second Column}
% SLIDE DOUBLE COLUMN NO TITLE 		:: \SliD{First Column}{Second Column}
% SLIDE ADVANCED WITH TITLE 			:: \SliAdvT{Title}{Content}
% SLIDE ADVANCED NO TITLE 			:: \SliAdv{Content}
% SLIDE ADVANCED DOUBLE WITH TITLE 	:: \SliAdvDT{Title}{First Column}{Second Column}
% SLIDE ADVANCED DOUBLE NO TITLE 	:: \SliAdvD{First Column}{Second Column}
% SLIDE BLACK						:: \Black{ <Content> }
% SLIDE WHITE						:: \White{ <Content> }
% ITEMIZATION 						:: \begin{itemize}  \iOn{First} \iTw {Second} \iTh{Third} \end{itemize}
% COMMENT TEXT				 		:: \note{<comment>}
% SECTION 							:: \secx{Section} | \secxx{Sub-Section}
% BOLD SPRACE COLOR				:: \bfs{<text>}
% TABLE OF CONTENT					:: \tocitem{<title>}{<content>}
% LEFT ALIGN EQUATION				:: \begin{flalign*}  & <equation> &   \end{flalign*}
% CENTER ALIGN EQUATION	S			:: \begin{gather*} <equations>  \end{gather*}
% SLASH								:: \slashed{<>}
% BAR								:: \barr{<letter>} instead of \bar{<letter>}
% THEREFORE						:: use \portanto (larger and bold}
% 2 or 3 MATH SYMBOLS				:: \overset{<up>}{<down>} &  \underset{<below>}{\overset{<above>}{<middle>}}  
% INSERT TEXT IN FORMULA			:: \ins{<text>}
% EXERCISE							:: \exe{<exercise #>}{<exercise text>}
% SUGGESTED READING BOX			:: \sug{<references>}
% CITATION							:: \cittex{<citation>}
% CITATION DOUBLE COLUMN 			:: \cittexD{<citation>}
% TEXT POSITION						:: \texpos{<Xcm>}{<Ycm>}{<text>} origin = center of slide : x right | y down
% REFERENCE AT BOTTOM  S/D SLIDE		:: \refbotS{<reference>} \refbotD{<reference>}
% HIDDEN SLIDE						:: \hid
% COLOR BOX 						:: \blu{blue} + \red{rec} + \yel{yellow} + \gre{green} + \bege{beige}
% FRAME 							:: \fra{sprace} \frab{blue} \frar{red} + \fray{yellow} + \frag{green}		
% FIGURE 							:: \img{X}{Y}{<scale>}{Figure.png} 
% FIGURE							:: \includegraphics[scale=<scale>]{Figures/.png}
% FIGURE DOUBLE SLIDE NO TITLE		::  \img{-4}{0.5}{<scale>}{Figure.png} % Image 1st half
%									::  \img{4}{0.5}{<scale>}{Figure.png} % Image 2nd half
% FIGURE DOUBLE SLIDE WITH TITLE		::  \img{-4}{0}{<scale>}{Figure.png} % Image 1st half
%									::  \img{4}{0}{<scale>}{Figure.png} % Image 2nd half
% INCLUDING SWF (Flash)				:: \usepackage{media9} and \includemedia >> USE ACROBAT <<
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
% FIRST SLIDE
\SliTit{{\LARGE Kernel Fisher Discriminant Analysis}}{Advanced Institute for Artificial Intelligence -- AI2}{https://advancedinstitute.ai}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
% SLIDE SUB-TITLE
%\SliSubTit{Sub-Title}{Description}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
%\SliSubSubTit{Sub-Sub-Title}{Description}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\SliT{Introdução}{

\justifying A técnica de \emph{Kernel Fisher Discriminant Analysis} - (KFDA) é uma \textbf{generalização não linear} para LDA. Novamente, faremos uso dos \emph{kernels} para tornar LDA uma técnica de projeção não linear. Inicialmente, iremos abordar a versão para classificação com duas classes.\newline

\justifying Seja, então, ${\cal X} = \{(\bm{x}_1,y_1),(\bm{x}_2,y_2),\ldots,(\bm{x}_z,y_z)\}$ um conjunto de dados tal que $\bm{x}_i\in\mathbb{R}^n$ e $\phi:\mathbb{R}^n\rightarrow\mathbb{R}^{n^\prime}$ uma função de mapeamento não linear tal que $n^\prime>n$. Ademais, temos que ${\cal Y}=\{\omega_1,\omega_2\}$ de tal forma que $y_i\in\mathbb{\cal Y}$. No KFDA busca-se maximizar o seguinte critério:

\begin{equation}
	J(\bm{w}) = \frac{\bm{w}^T\bm{S}^\phi_B\bm{w}}{\bm{w}^T\bm{S}^\phi_A\bm{w}}.
\end{equation}
Note que o critério é o mesmo que LDA. O que muda, porém, é a maneira com a qual temos que calcular as matrizes de espalhamento interclasses $\bm{S}^\phi_B$ e intraclasses $\bm{S}^\phi_A$.
}

\Sli{
\justifying Neste caso, temos que:

\begin{equation}
	\bm{S}^\phi_B = (\bm{\mu}_1^\phi-\bm{\mu}_2^\phi)(\bm{\mu}_1^\phi-\bm{\mu}_2^\phi)^T.
\end{equation}
Além disso, temos que:
\begin{equation}
	\bm{S}^\phi_A = \bm{S}^\phi_1+\bm{S}^\phi_2,
\end{equation}
em que
\begin{equation}
	\bm{S}^\phi_i = \sum_{\bm{x}_j\in\omega_i}(\phi(\bm{x}_j)-\bm{\mu}_i^\phi)(\phi(\bm{x}_j)-\bm{\mu}_i^\phi)^T,\ i\in\{1,2\}.
\end{equation}
Temos que a média de cada classe é dada por:
\begin{equation}
	\bm{\mu}_i^\phi = \frac{1}{m_i}\sum_{j=1}^{m_i}\phi(\bm{x}_j),\ i\in\{1,2\},
\end{equation}
em que $m_i$ denota a quantidade de elementos do conjunto de treinamento ${\cal X}_1\subset{\cal X}$ da classe $\omega_i$ tal que $|{\cal X}_1|=m=m_1+m_2$.
}

\Sli{
\justifying Existe um teorema importante que diz que \textbf{qualquer} vetor solução $\bm{w}\in\mathbb{R}^{n^\prime}$ para o problema de maximizar a Equação 1 deve fazer parte do espaço gerado por todas as amostras do conjunto de treinamento. Assim, temos que $\bm{w}$ é uma combinação linear das amostras de treinamento, ou seja:

\begin{equation}
\bm{w} = \sum_{i=1}^m\alpha_i\phi(\bm{x}_i),\ \ \forall \bm{x}_i\in{\cal X}_1.	
\end{equation}
}

\Sli{
\justifying Temos que a projeção de $\bm{\mu}^\phi_1$ na direção do vetor $\bm{w}$ é dada por:

\begin{align}\nonumber
	\bm{w}^T\bm{\mu}^\phi_1 &= \overbrace{\left(\sum_{i=1}^m\alpha_i\phi(\bm{x}_i)\right)^T}^{\bm{w}^T}\overbrace{\left(\frac{1}{m_1}\sum_{j=1}^{m_1}\phi(\bm{x}_j)\right)}^{\bm{\mu}^\phi_1}=\frac{1}{m_1}\sum_{i=1}^m\sum_{j=1}^{m_1}\alpha_i\overbrace{\phi(\bm{x}_i)^T\phi(\bm{x}_j)}^{\text{kernel}} \\
	 &= \frac{1}{m_1}\sum_{i=1}^m\sum_{j=1}^{m_1}\alpha_i K(\bm{x}_i,\bm{x}_j).
\end{align}
}

\Sli{
\justifying De maneira análoga, temos que a projeção de $\bm{\mu}^\phi_2$ na direção do vetor $\bm{w}$ é dada por:

\begin{align}\nonumber
	\bm{w}^T\bm{\mu}^\phi_2 &= \overbrace{\left(\sum_{i=1}^m\alpha_i\phi(\bm{x}_i)\right)^T}^{\bm{w}^T}\overbrace{\left(\frac{1}{m_2}\sum_{j=1}^{m_2}\phi(\bm{x}_j)\right)}^{\bm{\mu}^\phi_2}=\frac{1}{m_1}\sum_{i=1}^m\sum_{j=1}^{m_2}\alpha_i\overbrace{\phi(\bm{x}_i)^T\phi(\bm{x}_j)}^{\text{kernel}} \\
	 &= \frac{1}{m_2}\sum_{i=1}^m\sum_{j=1}^{m_2}\alpha_i K(\bm{x}_i,\bm{x}_j).
\end{align}

Note, então, que as projeções dependem da matriz de \emph{kernel} e $\bm{\alpha}$, e que $(\bm{w}^T\bm{\mu}^\phi_i)\in\mathbb{R}$, $i\in\{1,2\}$.
}

\Sli{
\justifying Seja $\bm{M}^1\in\mathbb{R}^m$ tal que:

\begin{equation}
	\bm{M}^1_i = \frac{1}{m_1}\sum_{j=1}^{m_1}K(\bm{x}_i,\bm{x}_j),
\end{equation}
em que $\bm{x}_j\in\omega_1$ e $\bm{x}_i\in{\cal X}_1$. Basicamente, a equação acima calcula a média da $i$-ésima linha da matriz de \emph{kernel} da classe $1$. O mesmo vale para o cálculo de $\bm{M}^2\in\mathbb{R}^m$, ou seja:

\begin{equation}
	\bm{M}^2_i = \frac{1}{m_2}\sum_{j=1}^{m_2}K(\bm{x}_i,\bm{x}_j).
\end{equation}
}

\Sli{
\justifying Desta forma, podemos reescrever a Equação 7 da seguinte forma:

\begin{align}\nonumber
	\bm{w}^T\bm{\mu}^\phi_1 &= \frac{1}{m_1}\sum_{i=1}^m\sum_{j=1}^{m_1}\alpha_i K(\bm{x}_i,\bm{x}_j)\\\nonumber
	&= \sum_{i=1}^m\frac{1}{m_1}\sum_{j=1}^{m_1}\alpha_i K(\bm{x}_i,\bm{x}_j)\\
	&= \sum_{i=1}^m\alpha_i\underbrace{\frac{1}{m_1}\sum_{j=1}^{m_1}K(\bm{x}_i,\bm{x}_j)}_{\bm{M}^1_i}\\\nonumber
	&= \sum_{i=1}^m\alpha_i\bm{M}^1_i = \bm{\alpha}\bm{M}^1,\nonumber
\end{align}
em que $\bm{\alpha}\in\mathbb{R}^m$.
}

\Sli{
\justifying De modo análogo, podemos reescrever a Equação 8 da seguinte forma:

\begin{align}\nonumber
	\bm{w}^T\bm{\mu}^\phi_2 &= \frac{1}{m_2}\sum_{i=1}^m\sum_{j=1}^{m_2}\alpha_i K(\bm{x}_i,\bm{x}_j)\\\nonumber
	&= \sum_{i=1}^m\frac{1}{m_2}\sum_{j=1}^{m_2}\alpha_i K(\bm{x}_i,\bm{x}_j)\\
	&= \sum_{i=1}^m\alpha_i\underbrace{\frac{1}{m_2}\sum_{j=1}^{m_2}K(\bm{x}_i,\bm{x}_j)}_{\bm{M}^2_i}\\\nonumber
	&= \sum_{i=1}^m\alpha_i\bm{M}^2_i = \bm{\alpha}\bm{M}^2.\nonumber
\end{align}
A ideia de toda essa manipulação é para reescrevermos a Equação 1 (Critério de Fisher).
}

\Sli{
Desta fora, conseguimos reescrever o numerador da Equação 1 da seguinte forma:

\begin{align}\nonumber
	\bm{w}^T\bm{S}^\phi_B\bm{w} &= \bm{w}^T(\bm{\mu}_1^\phi-\bm{\mu}_2^\phi)(\bm{\mu}_1^\phi-\bm{\mu}_2^\phi)^T\bm{w}\\\nonumber
	&= (\bm{w}^T\bm{\mu}_1^\phi-\bm{w}^T\bm{\mu}_2^\phi)({\bm{\mu}_1^\phi}^T\bm{w}-{\bm{\mu}_2^\phi}^T\bm{w})\\
	&= (\bm{\alpha}^T\bm{M}^1-\bm{\alpha}^T\bm{M}^2)({\bm{M}^1}^T\bm{\alpha}-{\bm{M}^2}^T\bm{\alpha})\\\nonumber
	&= \bm{\alpha}^T(\bm{M}^1-\bm{M}^2)(\bm{M}^1-\bm{M}^2)^T\bm{\alpha}\\
	&= \bm{\alpha}^T\bm{M}\bm{\alpha}\nonumber,
\end{align}
em que $\bm{M} = (\bm{M}^1-\bm{M}^2)(\bm{M}^1-\bm{M}^2)^T$.
}

\Sli{
\justifying Agora, reescreveremos o denominador da Equação 1, ou seja:

\begin{align}\nonumber
	\bm{w}^T\bm{S}^\phi_A\bm{w} &= \left(\sum_{i=1}^m\alpha_i\phi(\bm{x}_i)\right)^T\bm{S}^\phi_A\left(\sum_{l=1}^m\alpha_i\phi(\bm{x}_l)\right)\\
	&= \left(\sum_{i=1}^m\alpha_i\phi(\bm{x}_i)\right)^T\underbrace{\left(\sum_{j=1}^2\sum_{k=1}^{m_j}(\phi(\bm{x}_k)-\bm{\mu_j}^\phi)(\phi(\bm{x}_k)-\bm{\mu_j}^\phi)^T\right)}_{\bm{S}^\phi_A = \bm{S}^\phi_1+\bm{S}^\phi_2}\left(\sum_{l=1}^m\alpha_i\phi(\bm{x}_l)\right).
\end{align}
}

\Sli{
\justifying Agrupando os somatórios e realizando algumas simplificações, temos que:

\begin{equation}
	\bm{w}^T\bm{S}^\phi_A\bm{w} = \bm{\alpha}^T\bm{N}\bm{\alpha},
\end{equation}
em que $\bm{N} = \bm{N}_1+\bm{N}_2 = \underbrace{\bm{K}_1(\bm{I}_1-\bm{1}_{m_1})\bm{K}_1^T}_{\bm{N}_1}+\underbrace{\bm{K}_2(\bm{I}_2-\bm{1}_{m_2})\bm{K}_2^T}_{\bm{N}_2}$. Neste caso, temos que $\bm{K}_1\in\mathbb{R}^{m\times m_1}$ corresponde à matriz de \emph{kernel} da classe $\omega_1$, $\bm{I}_1\in\mathbb{R}^{m_1\times m_1}$ denota a matriz identidade correspondente à classe $\omega_1$ e $\bm{1}_{m_1}\in\mathbb{R}^{m_1\times m_1}$ representa a matriz com entradas $1/m_1$ também da classe $\omega_1$. De maneira análoga, podemos definir $\bm{K}_2\in\mathbb{R}^{m\times m_2}$, $\bm{I}_2\in\mathbb{R}^{m_2\times m_2}$ e $\bm{1}_{m_2}\in\mathbb{R}^{m_2\times m_2}$ como sendo as matrizes de \emph{kernel}, identidade e com entradas $1/m_2$ da classe $\omega_2$, respectivamente.
}

\Sli{
Desta forma, conseguimos reescrever a Equação 1 da seguinte forma:

\begin{equation}
	J(\bm{\alpha}) = \frac{\bm{\alpha}^T\bm{M}\bm{\alpha}}{\bm{\alpha}^T\bm{N}\bm{\alpha}}.
\end{equation}
A condição necessária para otimizarmos a equação acima é dada por:

\begin{equation}
	\frac{\partial J(\bm{\alpha})}{\partial \bm{\alpha}} = 0.
\end{equation}
A solução da Equação 17 é dada por:
\begin{equation}
	\frac{\partial J(\bm{\alpha})}{\partial \bm{\alpha}} \implies (\bm{N}^{-1}\bm{M})\bm{\alpha} = \lambda\bm{\alpha}.
\end{equation}
Assim sendo, a solução é dada pelo autovetor $\bm{\alpha}$ associado ao maior autovalor da matriz $(\bm{N}^{-1}\bm{M})$, que é baseada nas matrizes de \emph{kernel}.
}

\Sli{
Agora, dada uma amostra de teste $\bm{x}\in{\cal X}_2$, como realizamos a sua projeção LDA? Basta, então, projetarmos essa amostra no vetor $\bm{w}$ encontrado pela função de otimização dada pela Equação 16:

\begin{equation}
	\hat{\bm{x}} = \phi(\bm{x})^T\bm{w}.
\end{equation}
No entanto, a Equação 16 é escrita em termos de $\bm{\alpha}$, o que nos remete à Equação 6, ou seja:

\begin{equation}
		\hat{\bm{x}} = \phi(\bm{x})^T\bm{w} = \phi(\bm{x})^T\sum_{i=1}^m\alpha_i\phi(\bm{x}) = \sum_{i=1}^m\alpha_i\phi(\bm{x})^T\phi(\bm{x}) = \sum_{i=1}^m\alpha_i K(\bm{x},\bm{x}_i).
\end{equation}
Assim sendo, uma amostra de teste pode ser projetada por meio de uma combinação linear das funções de \emph{kernel} das amostras do conjunto de treinamento.
}

\SliT{KFDA Multiclasses}{

\justifying Podemos generalizar o KFDA para problemas multiclasses também, ou seja, ${\cal Y}=\{\omega_1,\omega_2,\ldots,\omega_c\}$. Neste caso, da Equação 13, temos que $\bm{M} = (\bm{M}^1-\bm{M}^2)(\bm{M}^1-\bm{M}^2)^T$. Podemos, então, redefinir a matriz $\bm{M}$ da seguinte forma:

\begin{equation}
	\bm{M} = \sum_{j=1}^cm_j(\bm{M}^j-\bm{M}^\ast)(\bm{M}^j-\bm{M}^\ast)^T,
\end{equation}
em que $\bm{M}^j,\bm{M}^\ast\in\mathbb{R}^m$ e são calculados da seguinte forma:

\begin{equation}
	M^j_i = \frac{1}{m_j}\sum_{k=1}^{m_j}K(\bm{x}_i,\bm{x}_k),
\end{equation}
em que $\bm{x}_k$ varia entre todas as amostras da classe $\omega_j$ e $\bm{x}_i$ varia entre todas as amostras de treinamento.
}

\Sli{
\justifying Já $\bm{M}^\ast$ pode ser calculado da seguinte forma:

\begin{equation}
	M^\ast_i = \frac{1}{m}\sum_{k=1}^{m}K(\bm{x}_i,\bm{x}_k),
\end{equation}
$\bm{x}_i$ e $\bm{x}_m$ variam entre todas as amostras de treinamento. Já a matriz $\bm{N} = \bm{N}_1+\bm{N}_2$ da Equação 15 também pode ser generalizada da seguinte forma:

\begin{equation}
	\bm{N} = \bm{N}_1+\bm{N}_2+\ldots+\bm{N}_c = \sum_{j=1}^c\bm{K}_j(\bm{I}-\bm{1}_{m_j})\bm{K}_j^T.
\end{equation}
}

\Sli{
\justifying Desta forma, a solução ótima para o KFDA multiclasses consiste, novamente, em resolvermos a Equação 18, ou seja, consiste em obter os $d$ autovetores associados aos $d$ maiores autovalores da matriz $\bm{N}^{-1}\bm{M}$. A nova base é então definida por:

\begin{equation}
	\bm{W} = [\alpha_1,\alpha_2,\ldots,\alpha_d],
\end{equation}
ou seja, cada coluna da matriz $\bm{W}$ representa um autovetor $\alpha_k$. Já a projeção de uma amostra $\bm{x}$ é dada por:

\begin{equation}
	\hat{\bm{x}} = \phi(\bm{x})^T\bm{w}_k = \sum_{i=1}^m\alpha_{ki}\phi(\bm{x})^T\phi(\bm{x}_i) = \sum_{i=1}^m\alpha_{ki}K(\bm{x},\bm{x}_i),
\end{equation}
em que $\bm{x}_i$ varia entre todas as amostras do conjunto de treinamento.
}

\Sli{
\justifying Um agradecimento especial ao \textbf{Prof. Alexandre Levada} do Centro de Ciências Exatas e de Tecnologia, Departamento de Computação, Universidade Federal de São Carlos, pelas notas de aula.
}

\end{document}