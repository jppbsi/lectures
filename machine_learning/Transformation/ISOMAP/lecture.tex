%Fiquemos com Deus e Nossa Senhora!
%Sao Jose de Cupertino rogai por nos que recorremos a vos!!!
%Honra teu Pai e tua Mãe!
% ### Uses XeLaTeX ### %
% ### Needs beamer-master ### %
\documentclass[aspectratio=169]{beamer} %. Aspect Ratio 16:9

\usetheme{AI2} % beamerthemeSprace.sty
\usepackage{algpseudocodex}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ragged2e,gensymb,bm,amsmath,amssymb}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sign}{sgn}

% DATA FOR FOOTER
\date{2021}
\title{- ISOMAP}
\institute{Advanced Institute for Artificial Intelligence (AI2)}

\begin{document}
% ####################################
% FIRST SLIDE 						:: \SliTit{This is the Title of the Talk}{A. B. Name}{Sprace}
% SUB-TITLE SLIDE 					:: \SliSubTit{<title>}{<explanation}
% SUB-SUB-TITLE SLIDE				:: \SliSubSubTit{<title>}{<explanation}
% SLIDE WITH TITLE 					:: \SliT{Title}{Content}
% SLIDE NO TITLE 						:: \Sli{Content} 
% SLIDE DOUBLE COLUMN WITH TITLE 	:: \SliDT{Title}{First Column}{Second Column}
% SLIDE DOUBLE COLUMN NO TITLE 		:: \SliD{First Column}{Second Column}
% SLIDE ADVANCED WITH TITLE 			:: \SliAdvT{Title}{Content}
% SLIDE ADVANCED NO TITLE 			:: \SliAdv{Content}
% SLIDE ADVANCED DOUBLE WITH TITLE 	:: \SliAdvDT{Title}{First Column}{Second Column}
% SLIDE ADVANCED DOUBLE NO TITLE 	:: \SliAdvD{First Column}{Second Column}
% SLIDE BLACK						:: \Black{ <Content> }
% SLIDE WHITE						:: \White{ <Content> }
% ITEMIZATION 						:: \begin{itemize}  \iOn{First} \iTw {Second} \iTh{Third} \end{itemize}
% COMMENT TEXT				 		:: \note{<comment>}
% SECTION 							:: \secx{Section} | \secxx{Sub-Section}
% BOLD SPRACE COLOR				:: \bfs{<text>}
% TABLE OF CONTENT					:: \tocitem{<title>}{<content>}
% LEFT ALIGN EQUATION				:: \begin{flalign*}  & <equation> &   \end{flalign*}
% CENTER ALIGN EQUATION	S			:: \begin{gather*} <equations>  \end{gather*}
% SLASH								:: \slashed{<>}
% BAR								:: \barr{<letter>} instead of \bar{<letter>}
% THEREFORE						:: use \portanto (larger and bold}
% 2 or 3 MATH SYMBOLS				:: \overset{<up>}{<down>} &  \underset{<below>}{\overset{<above>}{<middle>}}  
% INSERT TEXT IN FORMULA			:: \ins{<text>}
% EXERCISE							:: \exe{<exercise #>}{<exercise text>}
% SUGGESTED READING BOX			:: \sug{<references>}
% CITATION							:: \cittex{<citation>}
% CITATION DOUBLE COLUMN 			:: \cittexD{<citation>}
% TEXT POSITION						:: \texpos{<Xcm>}{<Ycm>}{<text>} origin = center of slide : x right | y down
% REFERENCE AT BOTTOM  S/D SLIDE		:: \refbotS{<reference>} \refbotD{<reference>}
% HIDDEN SLIDE						:: \hid
% COLOR BOX 						:: \blu{blue} + \red{rec} + \yel{yellow} + \gre{green} + \bege{beige}
% FRAME 							:: \fra{sprace} \frab{blue} \frar{red} + \fray{yellow} + \frag{green}		
% FIGURE 							:: \img{X}{Y}{<scale>}{Figure.png} 
% FIGURE							:: \includegraphics[scale=<scale>]{Figures/.png}
% FIGURE DOUBLE SLIDE NO TITLE		::  \img{-4}{0.5}{<scale>}{Figure.png} % Image 1st half
%									::  \img{4}{0.5}{<scale>}{Figure.png} % Image 2nd half
% FIGURE DOUBLE SLIDE WITH TITLE		::  \img{-4}{0}{<scale>}{Figure.png} % Image 1st half
%									::  \img{4}{0}{<scale>}{Figure.png} % Image 2nd half
% INCLUDING SWF (Flash)				:: \usepackage{media9} and \includemedia >> USE ACROBAT <<
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
% FIRST SLIDE
\SliTit{{\LARGE ISOMAP}}{Advanced Institute for Artificial Intelligence -- AI2}{https://advancedinstitute.ai}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
% SLIDE SUB-TITLE
%\SliSubTit{Sub-Title}{Description}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ###############################################################################
%\SliSubSubTit{Sub-Sub-Title}{Description}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\SliT{Introdução}{

\justifying A técnica \emph{Isometric Feature Mapping} (ISOMAP) é um algoritmo para redução não linear de dimensionalidade que baseia-se no arcabouço de \textbf{aprendizado de variedades} (\emph{manifold} learning). A principal diferença desse método para outros do tipo PCA (KPCA) e LDA/MDA (KFDA) é que aqui construimos uma relação de adjacência entre as  amostras.
}

\Sli{
\justifying \underline{Ideia geral}: construir um grafo unindo os vizinhos mais próximos, calcular os menores caminhos entre cada par de vértices e encontrar um mapeamento para o plano que preserve essas distâncias.

\justifying \underline{Hipótese}: caminhos mínimos em um grafo podem aproximar bem as distâncias geodésicas em espaços não Euclidianos (variedades).

\justifying \underline{Aprendizado de métricas}: métodos lineares falham em aprender uma medida de distância adequada na presença de não linearidades nos dados.

\begin{center}
\includegraphics[scale=0.23]{./figs/ISOMAP_Fig1.png}
\end{center}
}

\Sli{
\justifying O algoritmo do ISOMAP pode ser resumidos em três passos:

\begin{enumerate}
	\item Induzir o grafo $k$-nn a partir do conjunto de dados ${\cal X} = \{(\bm{x}_1,y_1),(\bm{x}_2,y_2,\ldots,(\bm{x}_m,y_m)\}$, em que $\bm{x}_i\in\mathbb{R}^n$ e $y\in{\cal Y}$ tal que ${\cal Y}=\{\omega_1,\omega_2,\ldots,\omega_c\}$ e $k<n$.
	\item Criar a matriz de distâncias ponto a ponto $\bm{D}\in\mathbb{R}^{m\times m}$ entre as amostras de ${\cal X}$. Desta forma, $D_{ij}$ representa o menor caminho entre as amostras $\bm{x}_i$ e $\bm{x}_j$.
	\item Encontrar o conjunto de pontos $\hat{{\cal X}}$ no espaço $\mathbb{R}^k$ tal que as distâncias originais sejam "preservadas". Esta etapa é realizada pela técnica MDS (\emph{Multidimensional Scaling}).
\end{enumerate}
Trata-se, portanto, de uma abordagem \textbf{global}, pois utiliza todas as amostras para calcular as distâncias. A técnica é baseada em um teorema que diz que caminhos mínimos em grafos $k$-nn são boas aproximações para as distâncias geodésicas (distâncias que respeitam as variedades).
}

\Sli{
\justifying Como podemos calcular essas distâncias geodésicas? Seja ${\cal G}=({\cal V},{\cal E},w)$ um grafo tal que ${\cal V}=\{\bm{v}_1,\bm{v}_2,\ldots,\bm{v}_m\}$ e ${\cal E}$ representam os conjuntos de vértices e arestas, respectivamente, e $w:{\cal V}\times{\cal V}\rightarrow\mathbb{R}^+$ uma função que calcula a distância entre dois nós. Note que os nós do grafos são compostos pelas amostras em ${\cal X}$, ou seja, $\bm{v}_i=\bm{x}_i$.

\justifying Temos que um caminho $P^\ast_{\bm{v}_1,\bm{v}_m}$ é dito ser ótimo se o seu custo $C$ é o menor possível, ou seja:

\begin{equation}
	C(P^\ast_{\bm{v}_1,\bm{v}_m}) = \sum_{i=1}^{m-1}w(\bm{v}_i,\bm{v}_{i+1}) = w(\bm{v}_2,\bm{v}_{2})+w(\bm{v}_2,\bm{v}_{3})+\ldots+w(\bm{v}_{n-1},\bm{v}_{m}), 
\end{equation}
em que $C(P^\ast_{\bm{v}_1,\bm{v}_m})$ se aproxima da distância geodésica entre $\bm{v}_1$ e $\bm{v}_m$.
}

\Sli{
\justifying Uma técnica bastante conhecida para calcular caminhos mínimos em grafos é chamada de \textbf{Algoritmo de Dijkstra}. Existe um teorema que mostra que este algoritmo sempre calcula a distância geodésica entre um nó \textbf{fonte} e demais nós do grafo. Vejamos as seguinte definições:

\begin{itemize}
	\item $\lambda(\bm{v})$: menor custo até o momento de alguma amostra $\bm{s}$ até $\bm{v}$.
	\item $\pi(\bm{v})$: predecessor de $\bm{v}$ no caminho de custo ótimo.
	\item ${\cal Q}$: fila de prioridades dos vértices.
\end{itemize}

}

\Sli{
Vejamos, agora, o algoritmo do Dijkstra. 
\begin{center}
\begin{minipage}{0.4\textwidth}
\begin{algorithmic}
\State \Call{Dijkstra}{${\cal G}$, $\bm{s}$}
	\For{each $\bm{v}\in{\cal V}$}
	\State{$\lambda(\bm{v})\leftarrow+\infty$}
	\State{$\pi(\bm{v})\leftarrow nil$}
	\EndFor
	\State{$\lambda(\bm{s})\leftarrow0$}
	\State{$\pi(\bm{s})\leftarrow nil$}
	\State{${\cal Q}\leftarrow{\cal V}$, ${\cal S}\leftarrow\emptyset$}
	\While{${\cal Q}\neq\emptyset$}
	\State{$\bm{u}\leftarrow Remove({\cal Q})$}
	\State{${\cal S}\leftarrow{\cal S}\cup\bm{u}$}
	\For{each $\bm{v}\in{\cal N}(\bm{u})$}
	\If{$\lambda(\bm{v})>\lambda(\bm{u})+w(\bm{u},\bm{v})$}
	\State{$\lambda(\bm{v})\leftarrow\lambda(\bm{u})+w(\bm{u},\bm{v})$}
	\State{$\pi(\bm{v})\leftarrow\bm{u}$}
	\EndIf
	\EndFor
	\EndWhile
\end{algorithmic}
\end{minipage}
\begin{minipage}{0.42\textwidth}
\includegraphics[scale=0.57]{./figs/ISOMAP_Fig2.png}
\end{minipage}%%% to prevent a space
\end{center}
}

\SliT{Multidimensional Scaling}{
\justifying Como fazemos o mapeamento para o espaço $\mathbb{R}^k$ ($k$ é escolhido pelo usuário)? Assim, dada a nossa matriz de distâncias geodésicas calculadas pelo algoritmo de Dijkstra, o objetivo é encontrar um novo mapeamento das amostras de tal forma que suas distâncias originais sejam preservadas. A solução para este problema é dada pela técnica MDS.
\justifying Lembrando que a distância $D_{ij}$ entre dois vetores $\bm{x}_i$ e $\bm{x}_j$ é dada por:

\begin{equation}
	D_{ij} = \norm{\bm{x}_i-\bm{x}_j}^2=(\bm{x}_i-\bm{x}_j)^T(\bm{x}_i-\bm{x}_j).
\end{equation}
Note que a matriz de distâncias $\bm{D}$ já foi calculada anteriormente.
}

\Sli{
\justifying O método MDS baseia-se na solução de dois subproblemas:

\begin{enumerate}
	\item Encontrar a matriz $\bm{B}\in\mathbb{R}^{m\times m}$ a partir de $\bm{D}$ e
	\item Recuperar (mapear) as coordenadas dos novos pontos a partir de $\bm{B}$.
\end{enumerate}
\justifying Note que $\bm{B}$ corresponde à matriz de produtos internos entre todas as amostras, ou seja, $B_{ij} = \bm{x}_i^T\bm{x}_j$. Vejamos, então, como resolver cada um dos subproblemas.
}

\Sli{
\justifying \underline{Subproblema 1}: encontrar $\bm{B}$ a partir de $\bm{D}$.\newline

\underline{Hipótese}: a média $\bm{\mu}\in\mathbb{R}^n$ dos dados é nula, ou seja:

\begin{equation}
	\bm{\mu} = \frac{1}{m}\sum_{i=1}^m\bm{x}_i = 0.
\end{equation}
Assumimos essa hipótese pois, caso contrário, teremos infinitas soluções para o problema (diferentes versões do espaço que estão transladadas, por exemplo). Ademais, aplicando a operação distributiva na Equação 2, temos que:

\begin{equation}
	D_{ij} = \bm{x}_i^T\bm{x}_i-2\bm{x}_i^T\bm{x}_j+\bm{x}_j^T\bm{x}_j.
\end{equation}
}

\Sli{
\justifying A partir da matriz $\bm{D}$, podemos calcular a média de uma coluna $s$ da seguinte maneira:
\vspace{-0.3cm}
\begin{align}\nonumber
	M_s &= \frac{1}{m}\sum_{i=1}^mD_{is} = \frac{1}{m}\sum_{i=1}^m\overbrace{(\bm{x}_i^T\bm{x}_i-2\bm{x}_i^T\bm{x}_s+\bm{x}_s^T\bm{x}_s)}^{D_{is}} \\\nonumber
	&= \frac{1}{m}\left(\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-2\sum_{i=1}^m\bm{x}_i^T\bm{x}_s+\sum_{i=1}^m\bm{x}_s^T\bm{x}_s\right)\\
	&= \frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-\frac{2}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_s+\frac{1}{m}\cancelto{\text{constante}}{\sum_{i=1}^m}\bm{x}_s^T\bm{x}_s\\\nonumber
	&= \frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-2\bm{x}_s\cancelto{0\text{ Equação 3}}{\frac{1}{m}\sum_{i=1}^m\bm{x}_i}+\frac{1}{m}m(\bm{x}_s^T\bm{x}_s)\\\nonumber
	&= \frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i+\bm{x}_s^T\bm{x}_s.
\end{align}
}

\Sli{
\justifying Analogamente, podemos calcular a média da linha $r$ da matriz $\bm{D}$ da seguinte maneira:
\vspace{-0.3cm}
\begin{align}\nonumber
	M_r &= \frac{1}{m}\sum_{j=1}^mD_{rj} = \frac{1}{m}\sum_{j=1}^m\overbrace{(\bm{x}_r^T\bm{x}_r-2\bm{x}_r^T\bm{x}_j+\bm{x}_j^T\bm{x}_j)}^{D_{rj}} \\\nonumber
	&= \frac{1}{m}\left(\sum_{j=1}^m\bm{x}_r^T\bm{x}_r-2\sum_{j=1}^m\bm{x}_r^T\bm{x}_j+\sum_{j=1}^m\bm{x}_j^T\bm{x}_j\right)\\
	&= \frac{1}{m}\cancelto{\text{constante}}{\sum_{j=1}^m}\bm{x}_r^T\bm{x}_r-\frac{2}{m}\sum_{j=1}^m\bm{x}_r^T\bm{x}_j+\frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j\\\nonumber
	&= \frac{1}{m}m(\bm{x}_r^T\bm{x}_r)-2\bm{x}_r\cancelto{0\text{ Equação 3}}{\frac{1}{m}\sum_{i=1}^m\bm{x}_j}+\frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j = \bm{x}_r^T\bm{x}_r+\frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j.\nonumber
\end{align}
Finalmente, podemos calcular a média $\bar{M}$ dos elementos da matriz $\bm{D}$ da seguinte forma:
}

\Sli{
\vspace{-0.7cm}
\begin{align}\nonumber
	\bar{M} &= \frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mD_{ij} = \frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^m\left(\bm{x}_i^T\bm{x}_i-2\bm{x}_i^T\bm{x}_j+\bm{x}_j^T\bm{x}_j\right)\\\nonumber
	&= \frac{1}{m^2}\sum_{i=1}^m\cancelto{m}{\sum_{j=1}^m}\bm{x}_i^T\bm{x}_i-\frac{2}{m^2}\sum_{i=1}^m\sum_{j=1}^m\bm{x}_i^T\bm{x}_j+\frac{1}{m^2}\cancelto{m}{\sum_{i=1}^m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j\\
	&= \frac{1}{m^2}m\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-\frac{2}{m^2}\sum_{i=1}^m\bm{x}_i^T\sum_{j=1}^m\bm{x}_j+\frac{1}{m^2}m\sum_{j=1}^m\bm{x}_j^T\bm{x}_j\\\nonumber
	&= \frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-\cancelto{0 \text{ Equação 3}}{\frac{2}{m}\sum_{i=1}^m\bm{x}_i^T}\frac{1}{m}\sum_{j=1}^m\bm{x}_j+\frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j\\
	&= \underbrace{\frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i+\frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j}_{\text{termos iguais}} = \frac{2}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i.\nonumber
\end{align}
}

\Sli{
\justifying Podemos definir o elemento $B_{ij}$ da matriz de produtos internos $\bm{B}$ da seguinte forma por meio da Equação 4:

\begin{equation}
	B_{ij} = \bm{x}_i^T\bm{x}_j = -\frac{1}{2}\left(D_{ij}-\bm{x}_i^T\bm{x}_i-\bm{x}^T\bm{x}_j\right).
\end{equation}

\justifying Da Equação 6, podemos isolar o termo $-\bm{x}_r^T\bm{x}_r$. Tornando $r=i$, temos que:

\begin{equation}
	-\bm{x}_i^T\bm{x}_i = \frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j-\frac{1}{m}\sum_{j=1}^mD_{ij}.
\end{equation}
}

\Sli{
\justifying Da Equação 5, podemos isolar o termo $-\bm{x}_s^T\bm{x}_s$. Tornando $s=j$, temos que:

\begin{equation}
	-\bm{x}_j^T\bm{x}_j = \frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-\frac{1}{m}\sum_{i=1}^mD_{ij}.
\end{equation}
Subtraindo-se as Equações 9 e 10, temos que:

\begin{align}\nonumber
	-\bm{x}_i^T\bm{x}_i-(-\bm{x}_j^T\bm{x}_j) &= \frac{1}{m}\sum_{j=1}^m\bm{x}_j^T\bm{x}_j-\frac{1}{m}\sum_{j=1}^mD_{ij}+\frac{1}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i-\frac{1}{m}\sum_{i=1}^mD_{ij}\\
	&= -\frac{1}{m}\sum_{j=1}^mD_{ij}-\frac{1}{m}\sum_{i=1}^mD_{ij}+\frac{2}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i\\\nonumber
\end{align}
}

\Sli{
\justifying Da Equação 7, podemos escrever:

\begin{equation}
	\frac{2}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i = \frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mD_{ij}.
\end{equation}
Lembrando que o nosso objetivo é reescrever os termos $b_{ij}$ da matriz de produtos internos $\bm{B}$ utilizando $D_{ij}$.
}

\Sli{

\justifying Assim sendo, podemos reescrever a Equação 8:

\begin{align}\nonumber
	B_{ij} &= \bm{x}_i^T\bm{x}_j = -\frac{1}{2}\left(D_{ij}\overbrace{-\bm{x}_i^T\bm{x}_i-\bm{x}^T\bm{x}_j}^{\text{Equação 11}}\right)\\
	&= -\frac{1}{2}\left(D_{ij}-\frac{1}{m}\sum_{j=1}^mD_{ij}-\frac{1}{m}\sum_{i=1}^mD_{ij}+\underbrace{\frac{2}{m}\sum_{i=1}^m\bm{x}_i^T\bm{x}_i}_{\text{Equação 12}}\right)\\\nonumber
	&= -\frac{1}{2}\left(D_{ij}-\frac{1}{m}\sum_{j=1}^mD_{ij}-\frac{1}{m}\sum_{i=1}^mD_{ij}+\frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mD_{ij}\right)\\\nonumber
\end{align}

}

\Sli{
\justifying Aplicando-se a operação distributiva na Equação 13, podemos criar a matriz $\bm{A}\in\mathbb{R}^{m\times m}$ e, fazendo $A_{ij} = -\frac{1}{2}D_{ij}$, temos as seguintes definições:

\begin{itemize}
	\item $\bar{A}_{i.} = \frac{1}{m}\sum_{j=1}^mA_{ij}$ (média na linha $i$)
	\item $\bar{A}_{.j} = \frac{1}{m}\sum_{i=1}^mA_{ij}$ (média na coluna $j$)
	\item $\bar{M} = \frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mA_{ij}$ (média da matriz $\bm{A}$)
\end{itemize}
}

\Sli{
\justifying Desta forma, podemos reescrever a Equação 13 da seguinte forma:

\begin{align}
	B_{ij} &= -\frac{1}{2}\left(D_{ij}-\frac{1}{m}\sum_{j=1}^mD_{ij}-\frac{1}{m}\sum_{i=1}^mD_{ij}+\frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mD_{ij}\right)\\
	&= A_{ij}-\bar{A}_{i.}-\bar{A}_{.j}+\bar{M}.\nonumber
\end{align}
Podemos mostrar, ainda, que:

\begin{equation}
	\bm{B} = \bm{H}\bm{A}\bm{H},
\end{equation}
em que
\begin{equation}
	\bm{H} = \bm{I}-\frac{1}{m}\bm{1}\bm{1}^T,
\end{equation} 
tal que $\bm{I}\in\mathbb{R}^{m\times m}$ é a matriz identidade e $\bm{1}\in\mathbb{R}^m$ é um vetor com todos os elementos iguais à $1$.
}

\Sli{
\justifying Temos, então, que:

\begin{equation}
	\bm{U} = \bm{1}\bm{1}^T=\begin{bmatrix}
		1 & 1 & \ldots & 1\\
		1 & 1 & \ldots & 1\\
		\vdots & \vdots & \ddots & \vdots\\
		1 & 1 & \ldots & 1\\
	\end{bmatrix}
\end{equation}
tal que $\bm{U}\in\mathbb{R}^{m\times m}$. Note que a Equação 15 nada mais é do que a forma matricial da Equação 14, ou seja:

\begin{align}\nonumber
	\bm{B} &= \bm{H}\bm{A}\bm{H} = \left(\bm{I}-\frac{1}{m}\bm{U}\right)\bm{A}\left(\bm{I}-\frac{1}{m}\bm{U}\right) = \left(\bm{A}-\frac{1}{m}\bm{U}\bm{A}\right)\left(\bm{I}-\frac{1}{m}\bm{U}\right)\\
	&= \bm{A}-\bm{A}\frac{1}{m}\bm{U}-\frac{1}{m}\bm{U}\bm{A}+\frac{1}{m^2}\bm{U}\bm{A}\bm{U}.
\end{align}
}

\Sli{
\justifying Conseguimos, então, resolver o subproblema 1, ou seja, encontrar a matriz $\bm{B}$. Falta, agora, resolvermos o subproblema 2, ou seja, recuperar as coordenadas $\hat{\bm{x}}_i\in\mathbb{R}^k$ a partir da matriz $\bm{B}$, $\forall \bm{x}_i\in{\cal X}$.\newline

\justifying Como $\bm{B}$ é a matriz dos produtos internos, podemos reescrevê-la da seguinte maneira:

\begin{equation}
	\bm{B} = \hat{\bm{X}}\hat{\bm{X}}^T,
\end{equation}
em que $\hat{\bm{X}}\in\mathbb{R}^{m\times k}$ é a matriz dos pontos projetados em $\mathbb{R}^k$. A matriz $\bm{B}$ possui certas propriedades, isto é, é simétrica, positiva semi-definida e possui rank $k$, o que significa que possui $k$ autovalores não negativos. A sua decomposição espectral é dada como segue: 

\begin{equation}
	\bm{B} = \bm{V}\bm{\Lambda}\bm{V}^T,
\end{equation}
em que $\bm{\Lambda}\in\mathbb{R}^{m\times m}$ é a matriz diagonal dos autovalores e $\bm{V}\in\mathbb{R}^{m\times m}$ é a matriz dos autovetores. 
}

\Sli{
\justifying Sem perda de generalidade, podemos considerar $\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_m$. Considerando apenas os $k$ maiores autovalores, podemos reescrever a Equação 20 da seguinte forma:

\begin{equation}
	\bm{B} = \bm{V}^\prime\bm{\Lambda}^\prime{\bm{V}^\prime}^T,
\end{equation}
em que $\bm{\Lambda}^\prime\in\mathbb{R}^{k\times k}$ é a matriz diagonal dos autovalores e $\bm{V}^\prime\in\mathbb{R}^{m\times k}$ denota a matriz dos autovetores. Da Equação 19, temos que:

\begin{equation}
	\bm{B} = \hat{\bm{X}}\hat{\bm{X}}^T = \bm{V}^\prime\bm{\Lambda}^\prime{\bm{V}^\prime}^T = \underbrace{\bm{V}^\prime{\bm{\Lambda}^\prime}^{1/2}}_{\hat{\bm{X}}}\overbrace{{\bm{\Lambda}^\prime}^{1/2}{\bm{V}^\prime}^T}^{\hat{\bm{X}}^T},
\end{equation}
em que ${\bm{\Lambda}^\prime}^{1/2}$ equivale à aplicação da raiz quadrada em todos os elementos de $\bm{\Lambda}^\prime$.
}

\Sli{
\justifying Desta forma, a matriz dos elementos projetados pode ser obtida da seguinte forma:

\begin{equation}
	\hat{\bm{X}} = \bm{V}^\prime{\bm{\Lambda}^\prime}^{1/2}.
\end{equation}
Vejamos, agora, o algoritmo MDS.

\begin{enumerate}
	\item Construir o grafo $k$-nn e aplicar o algoritmo de Dijkstra $m$ vezes mudando o nó fonte para calcular a matriz $\bm{D}$.
	\item Faça $\bm{A} = \frac{1}{2}\bm{D}$.
	\item Faça $\bm{H} = \bm{I}-\frac{1}{m}\bm{1}\bm{1}^T$.
	\item Calcule $\bm{B} = \bm{H}\bm{A}\bm{H}$.
	\item Encontrar os autovalores e autovetores de $\bm{B}$.
	\item Tomar os $k$ autovetores associados aos $k$ maiores autovalores de $\bm{B}$ para montar $\bm{V}^\prime$ e $\bm{\Lambda}^\prime$.
	\item Calcular $\hat{\bm{X}} = \bm{V}^\prime{\bm{\Lambda}^\prime}^{1/2}$.
\end{enumerate}
}

\Sli{
\justifying Vejamos algumas limitações do ISOMAP:

\begin{itemize}
	\item Não é supervisionado.
	\item Em dados não convexos, ou seja, quanto temos "buracos" nas variedades, ISOMAP pode não funcionar adequadamente.
\end{itemize}
}

\Sli{
\justifying Um agradecimento especial ao \textbf{Prof. Alexandre Levada} do Centro de Ciências Exatas e de Tecnologia, Departamento de Computação, Universidade Federal de São Carlos, pelas notas de aula.
}

\end{document}